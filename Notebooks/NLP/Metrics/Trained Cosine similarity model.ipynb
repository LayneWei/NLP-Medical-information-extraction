{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython #ENSURE cython package is installed on computer/canopy\n",
    "from gensim.models import phrases \n",
    "from gensim import corpora, models, similarities #calc all similarities at once, from http://radimrehurek.com/gensim/tut3.html\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "from statistics import mean\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import pandas as pd\n",
    "from string import ascii_letters, digits\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gensim.test.utils import datapath\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize_folder():\n",
    "    sentences = []\n",
    "    for fname in os.listdir(os.path.join(os.getcwd(), 'cTakes archive')): #go through all file names in current directory\n",
    "        if 'txt' in fname:\n",
    "            with open(os.path.join(os.getcwd(), 'cTakes archive', fname)) as file:\n",
    "                for l in file:\n",
    "                    l = l.translate(translator)\n",
    "                    l = re.sub(apas_error, \"'\", l)\n",
    "                    l = l.strip()\n",
    "                    l = l.lower()\n",
    "                    l = stripword(l)\n",
    "                    if len(l.split()) > 7:\n",
    "                        l = l + ' .'\n",
    "                        sentences.append(l.split())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_json():\n",
    "    sentences = []\n",
    "    with open('train.json') as json_f:\n",
    "        dataset = json.load(json_f)\n",
    "        for data in dataset:\n",
    "            for l in data['supports']:\n",
    "                l = l.translate(translator)\n",
    "                l = l.strip()\n",
    "                l = tokenizer.tokenize(l)\n",
    "                l = [stripword(n.lower()) for n in l]\n",
    "                for sent in l:\n",
    "                    if len(sent.split()) > 7:\n",
    "                        sent = sent + ' .'\n",
    "                        sentences.append(sent.split())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_json2():\n",
    "    sentences = []\n",
    "    with open('train1.0.json') as json_f:\n",
    "        dataset = json.load(json_f)\n",
    "        data = dataset[DATA_KEY]\n",
    "        for datum in data:\n",
    "            l = datum[DOC_KEY][CONTEXT_KEY]\n",
    "            l = l.translate(translator)\n",
    "            l = l.strip()\n",
    "            l = tokenizer.tokenize(l)\n",
    "            l = [stripword(n.lower()) for n in l]\n",
    "            for sent in l:\n",
    "                if len(sent.split()) > 7:\n",
    "                    sent = sent + ' .'\n",
    "                    sentences.append(sent.split())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def tokenize_file():\n",
    "    sentences = []\n",
    "    with open(os.path.join(os.getcwd(), 'cTakes archive', fname)) as file:\n",
    "        for i in range(0,7):\n",
    "            next(file, None)\n",
    "        for l in file:\n",
    "            l = l.translate(translator)\n",
    "            l = l.strip()\n",
    "            l = l.lower()\n",
    "            l = stripword(l)\n",
    "            sentences.append(l.split())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_xml():\n",
    "    sentences = []\n",
    "    for fname in os.listdir(os.path.join(os.getcwd(), 'cTakes archive/2try')): #go through all file names in current directory\n",
    "        if 'xml' in fname:\n",
    "            tree = ET.fromstring(open(os.path.join(os.getcwd(), 'cTakes archive/2try', fname)).read())\n",
    "            for node in tree.iter('TEXT'):\n",
    "                for elem in node.iter():\n",
    "                    l = elem.text.translate(translator)\n",
    "                    l = l.strip()\n",
    "                    l = l.lower()\n",
    "                    l = tokenizer.tokenize(l)\n",
    "                    l = [stripword(n) for n in l]\n",
    "                    for i, sent in enumerate(l):\n",
    "                        if i < 1:\n",
    "                            continue\n",
    "                        elif len(sent.split()) > 7:\n",
    "                            sent = sent + ' .'\n",
    "                            sentences.append(sent.split())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripword(word):\n",
    "    word = word.replace(\"[\", \"\")\n",
    "    word = word.replace(\"]\", \"\")\n",
    "    word = word.replace(\"'\", \"\")\n",
    "    word = word.replace(\"'\", \"\")\n",
    "    word = word.replace(\",\", \"\")\n",
    "    word = word.replace(\".\", \"\")\n",
    "    word = word.replace(\"_\", \"\")\n",
    "    word = word.replace(\":\", \"\")\n",
    "    word = word.replace(\"-\", \"\")\n",
    "    word = word.replace(\"*\", \"\")\n",
    "    word = word.replace(\"/\", \"\")\n",
    "    word = word.replace(\"(\", \"\")\n",
    "    word = word.replace(\")\", \"\")\n",
    "    word = word.replace(\"Â´\", \"\")\n",
    "    word = word.replace(\"`\", \"\")\n",
    "    word = word.replace(\";\", \"\")\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kote', 'oa']\n",
      "2728214\n"
     ]
    }
   ],
   "source": [
    "def tokenize():\n",
    "    translator = str.maketrans(ascii_letters, ascii_letters, digits)\n",
    "    sentences = []\n",
    "    for fname in os.listdir(os.path.join(os.getcwd(), 'cTakes archive')): #go through all file names in current directory\n",
    "        if 'txt' in fname:\n",
    "            with open(os.path.join(os.getcwd(), 'cTakes archive', fname)) as file:\n",
    "                for i in range(0,7):\n",
    "                    next(file, None)\n",
    "                for l in file:\n",
    "                    l = l.translate(translator)\n",
    "                    l = l.strip()\n",
    "                    l = l.lower()\n",
    "                    l = stripword(l)\n",
    "                    sentences.append(l.split())\n",
    "    return sentences\n",
    "\n",
    "sentences = tokenize()\n",
    "print(sentences[5])\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams = phrases.Phrases(sentences, min_count=3, delimiter=b' ')\n",
    "#sentences = [bigram[line] for line in sentences]\n",
    "#trigram = phrases.Phrases(sentences, min_count=1, delimiter=b' ')\n",
    "\n",
    "#trigram = phrases.Phraser(trigrams)\n",
    "\n",
    "#for sent in sentences:\n",
    "    #bigram = [b for b in bigrams[sent] if b.count(' ') == 1]\n",
    "    #trigram = [t for t in trigrams[bigram[sent]] if t.count(' ') == 2]\n",
    "\n",
    "#print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = phrases.Phrases(sentences) \n",
    "bigram = phrases.Phraser(bigram_transformer)\n",
    "\n",
    "\n",
    "\n",
    "trigram = phrases.Phrases(bigram_transformer[sentences])\n",
    "#trigram = phrases.Phraser(trigram_transformer)\n",
    "\n",
    "#for sentence in sentences:                \n",
    "        #print(trigram[bigram[sentence]])\n",
    "\n",
    "#print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentmodel= Word2Vec(trigram[bigram[sentences]], workers=4, sg=0,size=50, min_count=5,window=5, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentmodel.save(\"C:/Users/schaa/PycharmProjects/UCSF_NLP/cosine_model/cosine_similarity_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentmodel= Word2Vec.load(\"cosine_similarity_metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200983"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(currentmodel.wv.vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5999068 ,  0.35416883, -0.04330867,  0.02633828,  0.39897603,\n",
       "       -0.19057631, -0.13819359, -0.05821467, -0.01146783, -0.20452908,\n",
       "        0.05992888, -0.08144449,  0.17725985, -0.22243252,  0.24130572,\n",
       "        0.8107811 ,  0.01259795,  0.32264927,  0.2964318 ,  0.35275242,\n",
       "        0.29182708, -0.26796788, -0.03521559, -0.39380762, -0.00874169,\n",
       "       -0.34323993,  0.0502866 ,  0.04094986, -0.14384526, -0.0987082 ,\n",
       "        0.33888194,  0.33652577,  0.5721614 ,  0.02995919, -0.27039793,\n",
       "       -0.31809515,  0.11355288, -0.2792849 , -0.03718785,  0.00680792,\n",
       "       -0.00730454, -0.33719754, -0.32226714,  0.24901228, -0.43193877,\n",
       "       -0.44604075,  0.03743329, -0.43035948, -0.01268514, -0.62454164],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.wv['wife']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('strawberry', 0.887153685092926),\n",
       " ('nonfat', 0.8739331364631653),\n",
       " ('cocoa_powder', 0.8732395768165588),\n",
       " ('sugarsweetened', 0.8591521382331848),\n",
       " ('onion', 0.8565974235534668),\n",
       " ('noncaloric', 0.8525886535644531),\n",
       " ('wholemeal', 0.8516848683357239),\n",
       " ('drinks_containing', 0.850907564163208),\n",
       " ('blackcurrant', 0.8494446873664856),\n",
       " ('roasted', 0.8492937088012695)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentmodel.wv.most_similar('vodka', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2):\n",
    "    return 1 - spatial.distance.cosine(currentmodel.wv[word1], currentmodel.wv[word2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stripword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e71527686df3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mall_txt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_txt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'combined_procedures.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-e71527686df3>\u001b[0m in \u001b[0;36mextract_txt\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mrow13\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow13\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mrow13\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow13\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mrow13\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstripword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mrow3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mrow3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stripword' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_txt(filename):\n",
    "    all_txt = []\n",
    "    translator = str.maketrans(ascii_letters, ascii_letters, digits)\n",
    "    with open(filename) as csvfile:\n",
    "        csvdata = csv.reader(csvfile)\n",
    "        next(csvdata, None)\n",
    "        for row in csvdata:\n",
    "            row13 = row[13]\n",
    "            row3 = row[3]\n",
    "            row13 = row13.translate(translator)\n",
    "            row13 = row13.strip()\n",
    "            row13 = row13.lower()\n",
    "            row13 = stripword(row13)\n",
    "            row3 = row3.translate(translator)\n",
    "            row3 = row3.strip()\n",
    "            row3 = row3.lower()\n",
    "            row3 = stripword(row3)\n",
    "            all_txt.append([row13.split(), row3.split()])\n",
    "    return all_txt\n",
    "\n",
    "\n",
    "all_txt = extract_txt('combined_procedures.csv')\n",
    "    \n",
    "\n",
    "def combined_cosine_similarity(n):\n",
    "    cosine_accum = 0\n",
    "    number = 0\n",
    "    for word1 in all_txt[n][0]:\n",
    "        for word2 in all_txt[n][1]:\n",
    "            try:\n",
    "                cosine_accum += cosine_similarity(word1, word2)\n",
    "                number += 1\n",
    "            except:\n",
    "                cosine_accum += 0\n",
    "    #for i in all_txt[n][0]:\n",
    "        #for j in all_txt[n][1]:\n",
    "            #try:\n",
    "                #word1 = '_'.join(all_txt[n][0][i:i+2])\n",
    "                #word2 = '_'.join(all_txt[n][1][j:j+2])\n",
    "                #cosine_accum += cosine_similarity(all_txt[n][0][i], all_txt[n][1][j])\n",
    "                #number += 1\n",
    "            #except:\n",
    "                #cosine_accum += 0\n",
    "                \n",
    "                \n",
    "    cosine_average = cosine_accum / number\n",
    "    for word1 in all_txt[n][0]:\n",
    "        print(word1)\n",
    "    for word2 in all_txt[n][1]:\n",
    "        print(word2)\n",
    "    for i in range(0, len(all_txt[n][0])):\n",
    "        print('_'.join(all_txt[n][0][i:i+2]))\n",
    "    for j in range(0, len(all_txt[n][1])):\n",
    "        print('_'.join(all_txt[n][1][j:j+2]))\n",
    "    return cosine_average\n",
    "\n",
    "def combined_cosine_similarity2(n):\n",
    "    cosine_average = 0\n",
    "    count = 0\n",
    "    for m in range(0,len(all_txt[n][0])):\n",
    "        try:\n",
    "            word1 = '_'.join(all_txt[n][0][m:m+3])\n",
    "            word2 = '_'.join(all_txt[n][1][m:m+3])\n",
    "            cosine_average += cosine_similarity(word1, word2)\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if count == 0:\n",
    "        for m in range(0,len(all_txt[n][0])):\n",
    "            try:\n",
    "                word1 = '_'.join(all_txt[n][0][m:m+2])\n",
    "                word2 = '_'.join(all_txt[n][1][m:m+2])\n",
    "                cosine_average += cosine_similarity(word1, word2)\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "    if count == 0:\n",
    "        for word1 in all_txt[n][0]:\n",
    "            for word2 in all_txt[n][1]:\n",
    "                try:\n",
    "                    cosine_average += cosine_similarity(word1, word2)\n",
    "                    count += 1\n",
    "                except:\n",
    "                    pass\n",
    "    print('_'.join(all_txt[n][0]))\n",
    "    print('_'.join(all_txt[n][1]))\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return cosine_average/count\n",
    "    \n",
    "for n in range(0,100):\n",
    "    print(combined_cosine_similarity2(n))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12020156 -0.30863494 -0.20496017 -0.05184628 -0.17546077  0.14887759\n",
      " -0.21460165 -0.14561042  0.13152403  0.00774099 -0.0890884   0.17240329\n",
      "  0.15677413 -0.00983274  0.12491821  0.06888674  0.1877844   0.23370658\n",
      "  0.17949247 -0.10404465 -0.08324734 -0.1560629   0.02284857  0.06156752\n",
      "  0.03519148 -0.04163155 -0.00911897  0.06682156  0.03250984  0.00344628\n",
      " -0.19634135 -0.11050756  0.10098938  0.09158704  0.157167   -0.11486799\n",
      " -0.11026829 -0.0537328   0.08850987  0.05218657 -0.08690442  0.14819366\n",
      "  0.30944616 -0.03633654 -0.21226013  0.12385279 -0.00097142 -0.08887728\n",
      "  0.0700753  -0.10910447]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'combined_cosine_similarity2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-abc52b968203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdifference_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrentmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'staphylococcal_infections'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcurrentmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'staph'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdifference_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_cosine_similarity2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m93\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_cosine_similarity2' is not defined"
     ]
    }
   ],
   "source": [
    "difference_vector = currentmodel.wv['staphylococcal_infections'] - currentmodel.wv['staph']\n",
    "print(difference_vector)\n",
    "print(combined_cosine_similarity2(93))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
